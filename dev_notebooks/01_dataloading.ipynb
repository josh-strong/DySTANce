{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, re\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Get repository root directory\n",
    "# Try multiple methods to find the repo root\n",
    "cwd = Path.cwd()\n",
    "if (cwd / 'data').exists():\n",
    "    REPO_ROOT = cwd\n",
    "elif (cwd.parent / 'data').exists():\n",
    "    REPO_ROOT = cwd.parent\n",
    "else:\n",
    "    # Fallback: assume we're in dev_notebooks and go up one level\n",
    "    REPO_ROOT = cwd.parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce73d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_split_from_filename(fname: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Helper to figure out the split from the filename\n",
    "    \"\"\"\n",
    "    if fname.endswith('test.csv'):\n",
    "        return 'test'\n",
    "    elif fname.endswith('val.csv'):\n",
    "        return 'val'\n",
    "    elif fname.endswith('train.csv'):\n",
    "        return 'train'\n",
    "    else:\n",
    "        import warnings\n",
    "        warnings.warn(f\"Could not infer split from filename: {fname}\", UserWarning)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08361acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2046455/2064284380.py:13: UserWarning: Could not infer split from filename: dada\n",
      "  warnings.warn(f\"Could not infer split from filename: {fname}\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "infer_split_from_filename('dada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb087c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'resnet_mgca_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_mgca_pt_openi_train.csv',\n",
       "  'densenet121_res224_chex': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_chex_train.csv',\n",
       "  'densenet121_res224_all': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_all_train.csv',\n",
       "  'densenet_medical_mae_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet_medical_mae_pt_openi_train.csv',\n",
       "  'densenet_mocov2_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet_mocov2_pt_openi_train.csv',\n",
       "  'densenet121_res224_mimic_nb': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_mimic_nb_train.csv',\n",
       "  'densenet121_res224_nih': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_nih_train.csv',\n",
       "  'resnet_biovil_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_biovil_pt_openi_train.csv',\n",
       "  'densenet121_res224_mimic_ch': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_mimic_ch_train.csv',\n",
       "  'densenet121_res224_rsna': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_rsna_train.csv',\n",
       "  'evax_base_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_base_cxr__pt_openi_train.csv',\n",
       "  'densenet121_res224_pc': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_pc_train.csv',\n",
       "  'evax_small_chexpert_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_small_chexpert_pt_openi_train.csv',\n",
       "  'evax_tiny_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_tiny_cxr__pt_openi_train.csv',\n",
       "  'evax_small_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_small_cxr__pt_openi_train.csv',\n",
       "  'evax_tiny_chexpert_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_tiny_chexpert_pt_openi_train.csv',\n",
       "  'resnet50_res512_all': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet50_res512_all_train.csv',\n",
       "  'resnet_medklip_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_medklip_pt_openi_train.csv'},\n",
       " 'val': {'evax_small_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_small_cxr__pt_openi_val.csv',\n",
       "  'densenet121_res224_nih': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_nih_val.csv',\n",
       "  'densenet_mocov2_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet_mocov2_pt_openi_val.csv',\n",
       "  'evax_tiny_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_tiny_cxr__pt_openi_val.csv',\n",
       "  'densenet121_res224_mimic_ch': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_mimic_ch_val.csv',\n",
       "  'resnet_medklip_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_medklip_pt_openi_val.csv',\n",
       "  'densenet121_res224_chex': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_chex_val.csv',\n",
       "  'densenet121_res224_mimic_nb': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_mimic_nb_val.csv',\n",
       "  'evax_tiny_chexpert_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_tiny_chexpert_pt_openi_val.csv',\n",
       "  'densenet121_res224_rsna': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_rsna_val.csv',\n",
       "  'densenet121_res224_all': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_all_val.csv',\n",
       "  'evax_base_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_base_cxr__pt_openi_val.csv',\n",
       "  'evax_small_chexpert_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_small_chexpert_pt_openi_val.csv',\n",
       "  'densenet121_res224_pc': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_pc_val.csv',\n",
       "  'resnet_mgca_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_mgca_pt_openi_val.csv',\n",
       "  'densenet_medical_mae_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet_medical_mae_pt_openi_val.csv',\n",
       "  'resnet50_res512_all': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet50_res512_all_val.csv',\n",
       "  'resnet_biovil_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_biovil_pt_openi_val.csv'},\n",
       " 'test': {'evax_small_chexpert_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_small_chexpert_pt_openi_test.csv',\n",
       "  'densenet121_res224_chex': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_chex_test.csv',\n",
       "  'evax_small_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_small_cxr__pt_openi_test.csv',\n",
       "  'densenet121_res224_nih': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_nih_test.csv',\n",
       "  'resnet_mgca_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_mgca_pt_openi_test.csv',\n",
       "  'resnet50_res512_all': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet50_res512_all_test.csv',\n",
       "  'densenet_mocov2_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet_mocov2_pt_openi_test.csv',\n",
       "  'densenet121_res224_mimic_nb': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_mimic_nb_test.csv',\n",
       "  'densenet121_res224_pc': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_pc_test.csv',\n",
       "  'resnet_medklip_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_medklip_pt_openi_test.csv',\n",
       "  'densenet121_res224_mimic_ch': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_mimic_ch_test.csv',\n",
       "  'evax_tiny_chexpert_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_tiny_chexpert_pt_openi_test.csv',\n",
       "  'densenet_medical_mae_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet_medical_mae_pt_openi_test.csv',\n",
       "  'densenet121_res224_all': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_all_test.csv',\n",
       "  'densenet121_res224_rsna': '/home/kell6630/repos/DySTANce/data/openi/predictions/densenet121_res224_rsna_test.csv',\n",
       "  'resnet_biovil_pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/resnet_biovil_pt_openi_test.csv',\n",
       "  'evax_tiny_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_tiny_cxr__pt_openi_test.csv',\n",
       "  'evax_base_cxr__pt_openi': '/home/kell6630/repos/DySTANce/data/openi/predictions/evax_base_cxr__pt_openi_test.csv'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scan_prediction_files(pred_dir: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Helper to scan prediction files, infer the split from the filename, and return \n",
    "    registry of the form:   \n",
    "        registry[split][tool_name] = csv_path\n",
    "    \"\"\"\n",
    "    registry = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "    for fn in os.listdir(pred_dir):\n",
    "        if not fn.endswith(\".csv\"):\n",
    "            continue\n",
    "        split = infer_split_from_filename(fn)\n",
    "        if split is None:\n",
    "            continue\n",
    "        stem = os.path.splitext(fn)[0]\n",
    "        tool = re.sub(rf\"(_)?{split}(_)?$\", \"\", stem, flags=re.IGNORECASE).strip(\"_\")\n",
    "        registry[split][tool] = os.path.join(pred_dir, fn)\n",
    "    return registry\n",
    "\n",
    "reg = scan_prediction_files(str(REPO_ROOT / 'data' / 'openi' / 'predictions'))\n",
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_predictions_csv(\n",
    "    csv_path: str,\n",
    "    label_names: List[str],\n",
    "    id_candidates=(\"id\", \"filename\"),\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Takes in a csv path and a list of label names, and returns a dict of image_id -> [L] float array\n",
    "    This is for the multi-label task, where we treat it as l independent binary classifiers. \n",
    "    Note: Missing or unsupported labels are filled with 0.5!\n",
    "\n",
    "    Returns dict: image_id -> [L] float array\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    with open(csv_path, newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        header_lc = [h.lower().strip() for h in header]\n",
    "\n",
    "        id_idx = None\n",
    "        for cand in id_candidates:\n",
    "            if cand in header_lc:\n",
    "                id_idx = header_lc.index(cand)\n",
    "                break\n",
    "        if id_idx is None:\n",
    "            return out\n",
    "\n",
    "        label_idx = []\n",
    "        for l in label_names:\n",
    "            label_idx.append(header_lc.index(l.lower()) if l.lower() in header_lc else None)\n",
    "\n",
    "        for row in reader:\n",
    "            img_id = row[id_idx].replace(\".jpg\", \"\").strip()\n",
    "            vec = []\n",
    "            for j in label_idx:\n",
    "                if j is None:\n",
    "                    vec.append(0.5)\n",
    "                else:\n",
    "                    try:\n",
    "                        vec.append(float(row[j]))\n",
    "                    except Exception:\n",
    "                        vec.append(0.5)\n",
    "            out[img_id] = np.asarray(vec, dtype=np.float32)\n",
    "    return out\n",
    "df = pd.read_csv(REPO_ROOT / 'data' / 'openi' / 'predictions' / 'densenet_mocov2_pt_openi_train.csv')\n",
    "labels = df.drop(columns=['filename']).columns.tolist()\n",
    "out = read_predictions_csv(str(REPO_ROOT / 'data' / 'openi' / 'predictions' / 'densenet_mocov2_pt_openi_train.csv'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "def _fallback_to_tensor(img: Image.Image) -> torch.Tensor:\n",
    "    arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.stack([arr, arr, arr], axis=-1)\n",
    "    return torch.from_numpy(arr).permute(2, 0, 1)\n",
    "\n",
    "class OpenIRoutedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the OpenI dataset\n",
    "\n",
    "    Returns per-sample:\n",
    "      image        : Tensor[C,H,W]\n",
    "      gt           : Tensor[L]\n",
    "      tool_preds   : Tensor[M, L]\n",
    "      tool_mask    : Tensor[M, L]  (1 = tool valid for task, 0 invalide)\n",
    "      id           : str\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_csv: str,\n",
    "        images_dir: str,\n",
    "        predictions_registry: Dict[str, str],\n",
    "        label_names: List[str],\n",
    "        transform=None,\n",
    "        check_files=False,\n",
    "    ):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.label_names = label_names\n",
    "        self.L = len(label_names)\n",
    "\n",
    "        # --- Load labels ---\n",
    "        self.records = []\n",
    "        with open(label_csv, newline=\"\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)\n",
    "            hmap = {h.strip(): i for i, h in enumerate(header)}\n",
    "            for row in reader:\n",
    "                img_id = row[0].strip()\n",
    "                path = os.path.join(images_dir, f\"{img_id}.jpg\")\n",
    "                if check_files and not os.path.exists(path):\n",
    "                    continue\n",
    "                gt = [float(row[hmap[l]]) for l in label_names]\n",
    "                self.records.append({\n",
    "                    \"id\": img_id,\n",
    "                    \"path\": path,\n",
    "                    \"gt\": torch.tensor(gt, dtype=torch.float32)\n",
    "                })\n",
    "\n",
    "        # --- Load tool predictions ---\n",
    "        self.tool_names = sorted(predictions_registry.keys())\n",
    "        self.M = len(self.tool_names)\n",
    "\n",
    "        self.tool_preds = []\n",
    "        for tool in self.tool_names:\n",
    "            self.tool_preds.append(\n",
    "                read_predictions_csv(predictions_registry[tool], label_names)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "\n",
    "        img = Image.open(rec[\"path\"]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = _fallback_to_tensor(img)\n",
    "\n",
    "\n",
    "        preds = torch.full((self.M, self.L), 0.5)\n",
    "        mask  = torch.zeros((self.M, self.L))\n",
    "\n",
    "        for m, tool_dict in enumerate(self.tool_preds):\n",
    "            if rec[\"id\"] in tool_dict:\n",
    "                p = torch.from_numpy(tool_dict[rec[\"id\"]])\n",
    "                preds[m] = p\n",
    "                mask[m] = (torch.abs(p - 0.5) > 1e-4).float()\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"gt\": rec[\"gt\"],\n",
    "            \"tool_preds\": preds,\n",
    "            \"tool_mask\": mask,\n",
    "            \"id\": rec[\"id\"]\n",
    "        }\n",
    "\n",
    "\n",
    "tr_demo_dataset = OpenIRoutedDataset(\n",
    "    label_csv=str(REPO_ROOT / 'data' / 'openi' / 'labels' / 'Train.csv'),\n",
    "    images_dir=str(REPO_ROOT / 'data' / 'openi' / 'image'),\n",
    "    predictions_registry=reg['train'],\n",
    "    label_names=labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0acf5708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_demo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc32610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "class ContextManager:\n",
    "    \"\"\"\n",
    "    Manages the few-shot context sets used to describe tools in DySTANce.\n",
    "\n",
    "    Key idea (from the paper):\n",
    "    ------------------------\n",
    "    Tools are NOT identified by IDs or learned embeddings.\n",
    "    Instead, each tool E is represented only through its behaviour\n",
    "    on a small, task-specific context set:\n",
    "\n",
    "        D_E^t = {(x_b, y_b^t, m_E^t(x_b))}_{b=1}^{B_t}\n",
    "\n",
    "    This class is responsible for:\n",
    "      1) Constructing these context sets in a leakage-free way\n",
    "      2) Ensuring context examples are task- and tool-valid\n",
    "      3) Enforcing a strict separation between:\n",
    "           - data used to DESCRIBE tools (context)\n",
    "           - data used to TRAIN the router (routing set)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: OpenIRoutedDataset,\n",
    "        context_fraction: float = 0.1,\n",
    "        examples_per_tool: int = 32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : OpenIRoutedDataset\n",
    "            Full TRAINING dataset containing images, ground-truth labels,\n",
    "            tool predictions, and tool validity masks.\n",
    "\n",
    "        context_fraction : float\n",
    "            Fraction of the training data reserved EXCLUSIVELY for\n",
    "            tool description (context). These samples are never used\n",
    "            for routing loss computation.\n",
    "\n",
    "        examples_per_tool : int\n",
    "            Number of context examples B_t to sample per (tool, task)\n",
    "            when constructing the ANP summary.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.examples_per_tool = examples_per_tool\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) Split dataset indices into CONTEXT and ROUTING partitions\n",
    "        # ------------------------------------------------------------------\n",
    "        # This enforces the core invariance:\n",
    "        #   \"An image used to describe a tool is never used to train the router.\"\n",
    "        #\n",
    "        # This prevents information leakage and ensures the ANP summaries\n",
    "        # remain exogenous to the routing objective.\n",
    "        # ------------------------------------------------------------------\n",
    "        N = len(dataset)\n",
    "        perm = torch.randperm(N).tolist()  # random i.i.d. partition\n",
    "        split = int(context_fraction * N)\n",
    "\n",
    "        self.context_idx = perm[:split]    # used ONLY for tool descriptors\n",
    "        self.routing_idx = perm[split:]    # used ONLY for router training\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Pre-index valid context examples\n",
    "        # ------------------------------------------------------------------\n",
    "        # We build a lookup table:\n",
    "        #\n",
    "        #   (tool_idx, task_idx) -> [dataset indices]\n",
    "        #\n",
    "        # Only examples where:\n",
    "        #   - the tool actually produced a meaningful prediction\n",
    "        #   - for the specific task (label)\n",
    "        #\n",
    "        # are included.\n",
    "        #\n",
    "        # This is critical because many tools emit \"0.5\" for unsupported\n",
    "        # labels, which must NOT contaminate the context set.\n",
    "        # ------------------------------------------------------------------\n",
    "        self.pool = {}  # maps (tool_idx, task_idx) to list of dataset indices\n",
    "\n",
    "        for i in self.context_idx:\n",
    "            item = dataset[i]\n",
    "            mask = item[\"tool_mask\"]  # shape: [num_tools, num_tasks]\n",
    "\n",
    "            # Iterate over all (tool, task) pairs and record valid contexts\n",
    "            for t in range(dataset.M):\n",
    "                for l in range(dataset.L):\n",
    "                    if mask[t, l] > 0.5:\n",
    "                        # This example is informative for tool t on task l\n",
    "                        self.pool.setdefault((t, l), []).append(i)\n",
    "\n",
    "    def sample_context(self, tool_idx: int, task_idx: int):\n",
    "        \"\"\"\n",
    "        Samples a few-shot context set D_E^t for a specific tool and task.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (images, gt_labels, tool_predictions) or None\n",
    "\n",
    "        images           : Tensor[B, C, H, W]\n",
    "        gt_labels        : Tensor[B]\n",
    "        tool_predictions : Tensor[B]\n",
    "\n",
    "        This tuple corresponds exactly to:\n",
    "            (x_b, y_b^t, m_E^t(x_b))_{b=1}^{B_t}\n",
    "\n",
    "        If no valid context exists for (tool, task), returns None.\n",
    "        This signals that the tool has no observable behavior for this task.\n",
    "        \"\"\"\n",
    "\n",
    "        key = (tool_idx, task_idx)\n",
    "        candidates = self.pool.get(key, [])\n",
    "\n",
    "        # If the tool has never produced a valid prediction for this task,\n",
    "        # we cannot construct a meaningful context descriptor.\n",
    "        if len(candidates) == 0:\n",
    "            return None\n",
    "\n",
    "        # Randomly sample up to B_t context examples (few-shot, exchangeable)\n",
    "        idxs = random.sample(\n",
    "            candidates,\n",
    "            k=min(self.examples_per_tool, len(candidates))\n",
    "        )\n",
    "\n",
    "        imgs, gt, preds = [], [], []\n",
    "        for i in idxs:\n",
    "            item = self.dataset[i]\n",
    "\n",
    "            # Each context triple corresponds to:\n",
    "            #   image x_b\n",
    "            #   ground-truth label y_b^t\n",
    "            #   tool prediction m_E^t(x_b)\n",
    "            imgs.append(item[\"image\"])\n",
    "            gt.append(item[\"gt\"][task_idx])\n",
    "            preds.append(item[\"tool_preds\"][tool_idx, task_idx])\n",
    "\n",
    "        return (\n",
    "            torch.stack(imgs),\n",
    "            torch.stack(gt),\n",
    "            torch.stack(preds),\n",
    "        )\n",
    "\n",
    "    def routing_dataset(self):\n",
    "        \"\"\"\n",
    "        Returns the subset of the dataset used for training the router.\n",
    "\n",
    "        This subset is guaranteed to be disjoint from the context set,\n",
    "        ensuring no leakage between tool description and routing loss.\n",
    "        \"\"\"\n",
    "        return Subset(self.dataset, self.routing_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d18b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_demo_ctx_mgr = ContextManager(tr_demo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35393ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7e8fa6cb94c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_demo_ctx_mgr.routing_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c13eda15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.6078, 0.6392, 0.6627,  ..., 0.3176, 0.3294, 0.3804],\n",
       "           [0.6471, 0.6784, 0.7059,  ..., 0.3059, 0.3333, 0.3804],\n",
       "           [0.6275, 0.6431, 0.6627,  ..., 0.3137, 0.3255, 0.3725],\n",
       "           ...,\n",
       "           [0.2745, 0.2667, 0.2745,  ..., 0.2667, 0.2824, 0.3020],\n",
       "           [0.2745, 0.2627, 0.2745,  ..., 0.2706, 0.2863, 0.3059],\n",
       "           [0.2745, 0.2627, 0.2745,  ..., 0.2824, 0.2980, 0.3137]],\n",
       " \n",
       "          [[0.6078, 0.6392, 0.6627,  ..., 0.3176, 0.3294, 0.3804],\n",
       "           [0.6471, 0.6784, 0.7059,  ..., 0.3059, 0.3333, 0.3804],\n",
       "           [0.6275, 0.6431, 0.6627,  ..., 0.3137, 0.3255, 0.3725],\n",
       "           ...,\n",
       "           [0.2745, 0.2667, 0.2745,  ..., 0.2667, 0.2824, 0.3020],\n",
       "           [0.2745, 0.2627, 0.2745,  ..., 0.2706, 0.2863, 0.3059],\n",
       "           [0.2745, 0.2627, 0.2745,  ..., 0.2824, 0.2980, 0.3137]],\n",
       " \n",
       "          [[0.6078, 0.6392, 0.6627,  ..., 0.3176, 0.3294, 0.3804],\n",
       "           [0.6471, 0.6784, 0.7059,  ..., 0.3059, 0.3333, 0.3804],\n",
       "           [0.6275, 0.6431, 0.6627,  ..., 0.3137, 0.3255, 0.3725],\n",
       "           ...,\n",
       "           [0.2745, 0.2667, 0.2745,  ..., 0.2667, 0.2824, 0.3020],\n",
       "           [0.2745, 0.2627, 0.2745,  ..., 0.2706, 0.2863, 0.3059],\n",
       "           [0.2745, 0.2627, 0.2745,  ..., 0.2824, 0.2980, 0.3137]]],\n",
       " \n",
       " \n",
       "         [[[0.0157, 0.0157, 0.0157,  ..., 0.2471, 0.4706, 0.6039],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.2431, 0.4627, 0.5961],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.2392, 0.4588, 0.5882],\n",
       "           ...,\n",
       "           [0.0078, 0.0118, 0.0118,  ..., 0.2588, 0.4745, 0.6275],\n",
       "           [0.0118, 0.0118, 0.0118,  ..., 0.5725, 0.7176, 0.7176],\n",
       "           [0.0235, 0.0235, 0.0196,  ..., 0.7608, 0.8275, 0.7686]],\n",
       " \n",
       "          [[0.0157, 0.0157, 0.0157,  ..., 0.2471, 0.4706, 0.6039],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.2431, 0.4627, 0.5961],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.2392, 0.4588, 0.5882],\n",
       "           ...,\n",
       "           [0.0078, 0.0118, 0.0118,  ..., 0.2588, 0.4745, 0.6275],\n",
       "           [0.0118, 0.0118, 0.0118,  ..., 0.5725, 0.7176, 0.7176],\n",
       "           [0.0235, 0.0235, 0.0196,  ..., 0.7608, 0.8275, 0.7686]],\n",
       " \n",
       "          [[0.0157, 0.0157, 0.0157,  ..., 0.2471, 0.4706, 0.6039],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.2431, 0.4627, 0.5961],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.2392, 0.4588, 0.5882],\n",
       "           ...,\n",
       "           [0.0078, 0.0118, 0.0118,  ..., 0.2588, 0.4745, 0.6275],\n",
       "           [0.0118, 0.0118, 0.0118,  ..., 0.5725, 0.7176, 0.7176],\n",
       "           [0.0235, 0.0235, 0.0196,  ..., 0.7608, 0.8275, 0.7686]]],\n",
       " \n",
       " \n",
       "         [[[0.2275, 0.2196, 0.2235,  ..., 0.0627, 0.0549, 0.0667],\n",
       "           [0.2157, 0.2118, 0.2157,  ..., 0.0627, 0.0667, 0.0549],\n",
       "           [0.2157, 0.2078, 0.2118,  ..., 0.0745, 0.0627, 0.0588],\n",
       "           ...,\n",
       "           [0.2235, 0.2235, 0.2078,  ..., 0.0824, 0.0784, 0.1098],\n",
       "           [0.2353, 0.2392, 0.2275,  ..., 0.0941, 0.0863, 0.0980],\n",
       "           [0.2667, 0.2706, 0.2627,  ..., 0.1373, 0.1216, 0.1255]],\n",
       " \n",
       "          [[0.2275, 0.2196, 0.2235,  ..., 0.0627, 0.0549, 0.0667],\n",
       "           [0.2157, 0.2118, 0.2157,  ..., 0.0627, 0.0667, 0.0549],\n",
       "           [0.2157, 0.2078, 0.2118,  ..., 0.0745, 0.0627, 0.0588],\n",
       "           ...,\n",
       "           [0.2235, 0.2235, 0.2078,  ..., 0.0824, 0.0784, 0.1098],\n",
       "           [0.2353, 0.2392, 0.2275,  ..., 0.0941, 0.0863, 0.0980],\n",
       "           [0.2667, 0.2706, 0.2627,  ..., 0.1373, 0.1216, 0.1255]],\n",
       " \n",
       "          [[0.2275, 0.2196, 0.2235,  ..., 0.0627, 0.0549, 0.0667],\n",
       "           [0.2157, 0.2118, 0.2157,  ..., 0.0627, 0.0667, 0.0549],\n",
       "           [0.2157, 0.2078, 0.2118,  ..., 0.0745, 0.0627, 0.0588],\n",
       "           ...,\n",
       "           [0.2235, 0.2235, 0.2078,  ..., 0.0824, 0.0784, 0.1098],\n",
       "           [0.2353, 0.2392, 0.2275,  ..., 0.0941, 0.0863, 0.0980],\n",
       "           [0.2667, 0.2706, 0.2627,  ..., 0.1373, 0.1216, 0.1255]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.0784, 0.0784, 0.0784,  ..., 0.0784, 0.0824, 0.0745],\n",
       "           [0.0784, 0.0784, 0.0784,  ..., 0.0863, 0.0902, 0.0667],\n",
       "           [0.0784, 0.0784, 0.0784,  ..., 0.0706, 0.0745, 0.0902],\n",
       "           ...,\n",
       "           [0.6157, 0.6275, 0.6314,  ..., 0.5804, 0.6667, 0.7804],\n",
       "           [0.6157, 0.6314, 0.6392,  ..., 0.6039, 0.6980, 0.7647],\n",
       "           [0.6314, 0.6471, 0.6471,  ..., 0.6392, 0.7255, 0.7451]],\n",
       " \n",
       "          [[0.0784, 0.0784, 0.0784,  ..., 0.0784, 0.0824, 0.0745],\n",
       "           [0.0784, 0.0784, 0.0784,  ..., 0.0863, 0.0902, 0.0667],\n",
       "           [0.0784, 0.0784, 0.0784,  ..., 0.0706, 0.0745, 0.0902],\n",
       "           ...,\n",
       "           [0.6157, 0.6275, 0.6314,  ..., 0.5804, 0.6667, 0.7804],\n",
       "           [0.6157, 0.6314, 0.6392,  ..., 0.6039, 0.6980, 0.7647],\n",
       "           [0.6314, 0.6471, 0.6471,  ..., 0.6392, 0.7255, 0.7451]],\n",
       " \n",
       "          [[0.0784, 0.0784, 0.0784,  ..., 0.0784, 0.0824, 0.0745],\n",
       "           [0.0784, 0.0784, 0.0784,  ..., 0.0863, 0.0902, 0.0667],\n",
       "           [0.0784, 0.0784, 0.0784,  ..., 0.0706, 0.0745, 0.0902],\n",
       "           ...,\n",
       "           [0.6157, 0.6275, 0.6314,  ..., 0.5804, 0.6667, 0.7804],\n",
       "           [0.6157, 0.6314, 0.6392,  ..., 0.6039, 0.6980, 0.7647],\n",
       "           [0.6314, 0.6471, 0.6471,  ..., 0.6392, 0.7255, 0.7451]]],\n",
       " \n",
       " \n",
       "         [[[0.3216, 0.3255, 0.3373,  ..., 0.0980, 0.1176, 0.1608],\n",
       "           [0.3529, 0.3529, 0.3569,  ..., 0.0863, 0.1059, 0.1529],\n",
       "           [0.3843, 0.3843, 0.3882,  ..., 0.0824, 0.0980, 0.1451],\n",
       "           ...,\n",
       "           [0.4471, 0.4863, 0.5176,  ..., 0.7608, 0.7608, 0.8275],\n",
       "           [0.4706, 0.5137, 0.5490,  ..., 0.8314, 0.8471, 0.8902],\n",
       "           [0.5255, 0.5686, 0.6078,  ..., 0.9765, 0.9765, 0.9922]],\n",
       " \n",
       "          [[0.3216, 0.3255, 0.3373,  ..., 0.0980, 0.1176, 0.1608],\n",
       "           [0.3529, 0.3529, 0.3569,  ..., 0.0863, 0.1059, 0.1529],\n",
       "           [0.3843, 0.3843, 0.3882,  ..., 0.0824, 0.0980, 0.1451],\n",
       "           ...,\n",
       "           [0.4471, 0.4863, 0.5176,  ..., 0.7608, 0.7608, 0.8275],\n",
       "           [0.4706, 0.5137, 0.5490,  ..., 0.8314, 0.8471, 0.8902],\n",
       "           [0.5255, 0.5686, 0.6078,  ..., 0.9765, 0.9765, 0.9922]],\n",
       " \n",
       "          [[0.3216, 0.3255, 0.3373,  ..., 0.0980, 0.1176, 0.1608],\n",
       "           [0.3529, 0.3529, 0.3569,  ..., 0.0863, 0.1059, 0.1529],\n",
       "           [0.3843, 0.3843, 0.3882,  ..., 0.0824, 0.0980, 0.1451],\n",
       "           ...,\n",
       "           [0.4471, 0.4863, 0.5176,  ..., 0.7608, 0.7608, 0.8275],\n",
       "           [0.4706, 0.5137, 0.5490,  ..., 0.8314, 0.8471, 0.8902],\n",
       "           [0.5255, 0.5686, 0.6078,  ..., 0.9765, 0.9765, 0.9922]]],\n",
       " \n",
       " \n",
       "         [[[0.2196, 0.2196, 0.2196,  ..., 0.2941, 0.3765, 0.4510],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.2824, 0.3686, 0.4471],\n",
       "           [0.1922, 0.1922, 0.1882,  ..., 0.2667, 0.3569, 0.4392],\n",
       "           ...,\n",
       "           [0.1882, 0.1843, 0.1804,  ..., 0.2157, 0.2627, 0.3176],\n",
       "           [0.1882, 0.1882, 0.1804,  ..., 0.2196, 0.2745, 0.3412],\n",
       "           [0.1882, 0.1843, 0.1804,  ..., 0.3176, 0.4039, 0.4824]],\n",
       " \n",
       "          [[0.2196, 0.2196, 0.2196,  ..., 0.2941, 0.3765, 0.4510],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.2824, 0.3686, 0.4471],\n",
       "           [0.1922, 0.1922, 0.1882,  ..., 0.2667, 0.3569, 0.4392],\n",
       "           ...,\n",
       "           [0.1882, 0.1843, 0.1804,  ..., 0.2157, 0.2627, 0.3176],\n",
       "           [0.1882, 0.1882, 0.1804,  ..., 0.2196, 0.2745, 0.3412],\n",
       "           [0.1882, 0.1843, 0.1804,  ..., 0.3176, 0.4039, 0.4824]],\n",
       " \n",
       "          [[0.2196, 0.2196, 0.2196,  ..., 0.2941, 0.3765, 0.4510],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.2824, 0.3686, 0.4471],\n",
       "           [0.1922, 0.1922, 0.1882,  ..., 0.2667, 0.3569, 0.4392],\n",
       "           ...,\n",
       "           [0.1882, 0.1843, 0.1804,  ..., 0.2157, 0.2627, 0.3176],\n",
       "           [0.1882, 0.1882, 0.1804,  ..., 0.2196, 0.2745, 0.3412],\n",
       "           [0.1882, 0.1843, 0.1804,  ..., 0.3176, 0.4039, 0.4824]]]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0.5055, 0.6019, 0.1584, 0.1418, 0.4049, 0.3627, 0.4773, 0.0984, 0.5119,\n",
       "         0.5192, 0.5092, 0.2898, 0.3352, 0.6483, 0.5428, 0.5109, 0.5819, 0.1786,\n",
       "         0.0208, 0.0152, 0.0496, 0.5235, 0.1700, 0.0253, 0.0743, 0.0245, 0.2516,\n",
       "         0.0979, 0.4100, 0.1126, 0.0741, 0.1651]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_demo_ctx_mgr.sample_context(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd094be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch is list of dicts\n",
    "# batch = next(iter(train_loader))\n",
    "\n",
    "# # sample a task\n",
    "# task_idx = torch.randint(0, L, ()).item()\n",
    "\n",
    "# # slice task-specific view\n",
    "# preds = batch[\"tool_preds\"][:, :, task_idx]    # [B, M]\n",
    "# mask  = batch[\"tool_mask\"][:, :, task_idx]     # [B, M]\n",
    "# gt    = batch[\"gt\"][:, task_idx]                # [B]\n",
    "\n",
    "# # hard mask invalid tools BEFORE softmax\n",
    "# router_logits[mask == 0] = -1e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c31682",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â examples of data loading\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "PREDICTIONS_DIR = str(REPO_ROOT / 'data' / 'openi' / 'predictions')\n",
    "\n",
    "registry_all = scan_prediction_files(PREDICTIONS_DIR)\n",
    "\n",
    "# Example tool split (train on DenseNet + EVA-X, test on ResNets)\n",
    "train_tools = [t for t in registry_all[\"train\"] if \"resnet\" not in t]\n",
    "test_tools  = [t for t in registry_all[\"train\"] if \"resnet\" in t]\n",
    "\n",
    "train_registry = {t: registry_all[\"train\"][t] for t in train_tools}\n",
    "val_registry   = {t: registry_all[\"val\"][t]   for t in train_tools}\n",
    "test_registry  = {t: registry_all[\"test\"][t]  for t in test_tools}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1bd7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"Atelectasis\", \"Consolidation\", \"Infiltration\", \"Pneumothorax\",\n",
    "    \"Edema\", \"Emphysema\", \"Fibrosis\", \"Effusion\", \"Pneumonia\",\n",
    "    \"Pleural_Thickening\", \"Cardiomegaly\", \"Nodule\", \"Mass\", \"Hernia\",\n",
    "    \"Lung Lesion\", \"Fracture\", \"Lung Opacity\", \"Enlarged Cardiomediastinum\"\n",
    "]\n",
    "\n",
    "train_dataset_full = OpenIRoutedDataset(\n",
    "    label_csv=str(REPO_ROOT / 'data' / 'openi' / 'labels' / 'Train.csv'),\n",
    "    images_dir=str(REPO_ROOT / 'data' / 'openi' / 'image'),\n",
    "    predictions_registry=train_registry,\n",
    "    label_names=label_names,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "val_dataset = OpenIRoutedDataset(\n",
    "    label_csv=str(REPO_ROOT / 'data' / 'openi' / 'labels' / 'Valid.csv'),\n",
    "    images_dir=str(REPO_ROOT / 'data' / 'openi' / 'image'),\n",
    "    predictions_registry=val_registry,\n",
    "    label_names=label_names,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = OpenIRoutedDataset(\n",
    "    label_csv=str(REPO_ROOT / 'data' / 'openi' / 'labels' / 'Test.csv'),\n",
    "    images_dir=str(REPO_ROOT / 'data' / 'openi' / 'image'),\n",
    "    predictions_registry=test_registry,\n",
    "    label_names=label_names,\n",
    "    transform=transform,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcef8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_mgr = ContextManager(\n",
    "    dataset=train_dataset_full,\n",
    "    context_fraction=0.1,        # 10% reserved for context\n",
    "    examples_per_tool=32,        # B_t\n",
    ")\n",
    "train_dataset = ctx_mgr.routing_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f13ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72c36ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "num_tasks = len(label_names)\n",
    "num_tools = train_dataset_full.M\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 1. Sample a task (label) for this routing step\n",
    "        # -------------------------------------------------\n",
    "        task_idx = random.randint(0, num_tasks - 1)\n",
    "\n",
    "        images = batch[\"image\"]                 # [B, C, H, W]\n",
    "        gt     = batch[\"gt\"][:, task_idx]       # [B]\n",
    "        preds  = batch[\"tool_preds\"][:, :, task_idx]  # [B, M]\n",
    "        mask   = batch[\"tool_mask\"][:, :, task_idx]   # [B, M]\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 2. Sample context for each tool (ANP input)\n",
    "        # -------------------------------------------------\n",
    "        context_per_tool = []\n",
    "\n",
    "        for tool_idx in range(num_tools):\n",
    "            ctx = ctx_mgr.sample_context(tool_idx, task_idx)\n",
    "\n",
    "            if ctx is None:\n",
    "                context_per_tool.append(None)\n",
    "            else:\n",
    "                ctx_imgs, ctx_gt, ctx_preds = ctx\n",
    "                context_per_tool.append({\n",
    "                    \"images\": ctx_imgs,      # [B_t, C, H, W]\n",
    "                    \"gt\": ctx_gt,            # [B_t]\n",
    "                    \"preds\": ctx_preds,      # [B_t]\n",
    "                })\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 3. Forward pass (router + ANP)\n",
    "        # -------------------------------------------------\n",
    "        # router_logits = router(images, context_per_tool, task_idx)\n",
    "        #\n",
    "        # IMPORTANT:\n",
    "        # Mask invalid tools BEFORE softmax\n",
    "        #\n",
    "        # router_logits[mask == 0] = -1e9\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 4. Compute comp-sum loss (task-specific)\n",
    "        # -------------------------------------------------\n",
    "        # loss = comp_sum_loss(router_logits, preds, gt, mask)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6556fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "router.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        task_idx = random.randint(0, num_tasks - 1)\n",
    "\n",
    "        images = batch[\"image\"]\n",
    "        gt     = batch[\"gt\"][:, task_idx]\n",
    "        preds  = batch[\"tool_preds\"][:, :, task_idx]\n",
    "        mask   = batch[\"tool_mask\"][:, :, task_idx]\n",
    "\n",
    "        # Sample context exactly as during training\n",
    "        context_per_tool = [\n",
    "            ctx_mgr.sample_context(t, task_idx)\n",
    "            for t in range(num_tools)\n",
    "        ]\n",
    "\n",
    "        # router_logits = router(images, context_per_tool, task_idx)\n",
    "        # router_logits[mask == 0] = -1e9\n",
    "        # evaluate routing decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bbe4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eal2d-t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
