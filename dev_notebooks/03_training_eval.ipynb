{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e2c90aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "import imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73f1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Paths\n",
    "# ------------------------------------------------------------\n",
    "DATA_ROOT = \"/home/kell6630/repos/DySTANce//data/openi\"\n",
    "LABELS_DIR = f\"{DATA_ROOT}/labels\"\n",
    "IMAGES_DIR = f\"{DATA_ROOT}/image\"\n",
    "PRED_DIR   = f\"{DATA_ROOT}/predictions\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Labels (tasks)\n",
    "# ------------------------------------------------------------\n",
    "label_names = [\n",
    "    \"Atelectasis\", \"Consolidation\", \"Infiltration\", \"Pneumothorax\",\n",
    "    \"Edema\", \"Emphysema\", \"Fibrosis\", \"Effusion\", \"Pneumonia\",\n",
    "    \"Pleural_Thickening\", \"Cardiomegaly\", \"Nodule\", \"Mass\", \"Hernia\",\n",
    "    \"Lung Lesion\", \"Fracture\", \"Lung Opacity\", \"Enlarged Cardiomediastinum\"\n",
    "]\n",
    "num_tasks = len(label_names)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Tool registry\n",
    "# ------------------------------------------------------------\n",
    "registry_all = imports.scan_prediction_files(PRED_DIR)\n",
    "\n",
    "# Example split: train on non-resnet tools\n",
    "train_tools = [t for t in registry_all[\"train\"] if \"resnet\" not in t]\n",
    "\n",
    "train_registry = {t: registry_all[\"train\"][t] for t in train_tools}\n",
    "val_registry   = {t: registry_all[\"val\"][t]   for t in train_tools}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Datasets\n",
    "# ------------------------------------------------------------\n",
    "train_dataset_full = imports.OpenIRoutedDataset(\n",
    "    label_csv=f\"{LABELS_DIR}/Train.csv\",\n",
    "    images_dir=IMAGES_DIR,\n",
    "    predictions_registry=train_registry,\n",
    "    label_names=label_names,\n",
    "    transform=None,  # assume tensor conversion inside dataset\n",
    ")\n",
    "\n",
    "val_dataset = imports.OpenIRoutedDataset(\n",
    "    label_csv=f\"{LABELS_DIR}/Valid.csv\",\n",
    "    images_dir=IMAGES_DIR,\n",
    "    predictions_registry=val_registry,\n",
    "    label_names=label_names,\n",
    "    transform=None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de9c643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_mgr = imports.ContextManager(\n",
    "    dataset=train_dataset_full,\n",
    "    context_fraction=0.1,      # 10% context\n",
    "    examples_per_tool=32,      # B_t\n",
    ")\n",
    "\n",
    "train_dataset = ctx_mgr.routing_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07b777e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dcde5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = imports.DySTANceRouter(\n",
    "    num_tasks=num_tasks,\n",
    "    vocab_size=1000,   # dummy vocab size for now\n",
    "    hidden_dim=256,\n",
    ").to(device)\n",
    "\n",
    "criterion = imports.DySTANceLoss(\n",
    "    surrogate_type=\"logistic\",\n",
    "    lambda_entropy=0.05,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "\n",
    "def build_context_tensors(ctx_mgr, task_idx, device):\n",
    "    \"\"\"\n",
    "    Builds task-conditional context tensors for all tools.\n",
    "\n",
    "    Returns:\n",
    "        ctx_img_feat : [M, C, Dx] on `device`\n",
    "        ctx_gt       : [M, C]     on `device`\n",
    "        ctx_pred     : [M, C]     on `device`\n",
    "    \"\"\"\n",
    "    ctx_img_feats = []\n",
    "    ctx_gts = []\n",
    "    ctx_preds = []\n",
    "\n",
    "    M = ctx_mgr.dataset.M\n",
    "    C = ctx_mgr.examples_per_tool\n",
    "\n",
    "    for tool_idx in range(M):\n",
    "        ctx = ctx_mgr.sample_context(tool_idx, task_idx)\n",
    "\n",
    "        if ctx is None:\n",
    "            # No valid context for this tool-task pair\n",
    "            ctx_img_feats.append(\n",
    "                torch.zeros(C, model.img_dim, device=device)\n",
    "            )\n",
    "            ctx_gts.append(\n",
    "                torch.zeros(C, device=device)\n",
    "            )\n",
    "            ctx_preds.append(\n",
    "                torch.zeros(C, device=device)\n",
    "            )\n",
    "        else:\n",
    "            imgs, gt, preds = ctx\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            gt = gt.to(device)\n",
    "            preds = preds.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feats = model.extract_img_feat(imgs)  # [C, Dx]\n",
    "\n",
    "            ctx_img_feats.append(feats)\n",
    "            ctx_gts.append(gt)\n",
    "            ctx_preds.append(preds)\n",
    "\n",
    "    return (\n",
    "        torch.stack(ctx_img_feats, dim=0),  # [M, C, Dx]\n",
    "        torch.stack(ctx_gts, dim=0),        # [M, C]\n",
    "        torch.stack(ctx_preds, dim=0),      # [M, C]\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b2c15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, ctx_mgr, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        images = batch[\"image\"].to(device)        # [B, 3, H, W]\n",
    "        gt_all = batch[\"gt\"].to(device)           # [B, L]\n",
    "        preds_all = batch[\"tool_preds\"].to(device)  # [B, M, L]\n",
    "        mask_all = batch[\"tool_mask\"].to(device)    # [B, M, L]\n",
    "\n",
    "        B = images.size(0)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 1) Sample a task uniformly\n",
    "        # ------------------------------------------------------------\n",
    "        task_idx = random.randint(0, num_tasks - 1)\n",
    "        task_ids = torch.full((B,), task_idx, device=device, dtype=torch.long)\n",
    "\n",
    "        # Task-conditional slices\n",
    "        gt = gt_all[:, task_idx]                 # [B]\n",
    "        tool_preds = preds_all[:, :, task_idx]   # [B, M]\n",
    "        tool_mask  = mask_all[:, :, task_idx]    # [B, M]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 2) Build context for this task\n",
    "        # ------------------------------------------------------------\n",
    "        ctx_img_feat, ctx_gt, ctx_pred = build_context_tensors(\n",
    "            ctx_mgr, task_idx, device\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 3) Forward pass\n",
    "        # ------------------------------------------------------------\n",
    "        scores = model(\n",
    "            images=images,\n",
    "            text_tokens=torch.zeros((B, 1), dtype=torch.long, device=device),  # dummy text\n",
    "            task_idx=task_ids,\n",
    "            tool_preds=tool_preds,\n",
    "            ctx_img_feat=ctx_img_feat,\n",
    "            ctx_gt=ctx_gt,\n",
    "            ctx_pred=ctx_pred,\n",
    "            tool_mask=tool_mask,\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 4) Compute costs (classification task)\n",
    "        # c_E = 1 - confidence on true label\n",
    "        # ------------------------------------------------------------\n",
    "        tool_costs = 1.0 - tool_preds  # [B, M]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 5) Loss + backward\n",
    "        # ------------------------------------------------------------\n",
    "        loss, logs = criterion(scores, tool_costs, tool_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_loss / max(1, total_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f400aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, ctx_mgr):\n",
    "    model.eval()\n",
    "\n",
    "    total_regret = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Validation\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        gt_all = batch[\"gt\"].to(device)\n",
    "        preds_all = batch[\"tool_preds\"].to(device)\n",
    "        mask_all = batch[\"tool_mask\"].to(device)\n",
    "\n",
    "        B = images.size(0)\n",
    "\n",
    "        task_idx = random.randint(0, num_tasks - 1)\n",
    "        task_ids = torch.full((B,), task_idx, device=device, dtype=torch.long)\n",
    "\n",
    "        tool_preds = preds_all[:, :, task_idx]\n",
    "        tool_mask  = mask_all[:, :, task_idx]\n",
    "\n",
    "        ctx_img_feat, ctx_gt, ctx_pred = build_context_tensors(\n",
    "            ctx_mgr, task_idx, device\n",
    "        )\n",
    "\n",
    "        scores = model(\n",
    "            images,\n",
    "            torch.zeros((B, 1), dtype=torch.long, device=device),\n",
    "            task_ids,\n",
    "            tool_preds,\n",
    "            ctx_img_feat,\n",
    "            ctx_gt,\n",
    "            ctx_pred,\n",
    "            tool_mask,\n",
    "        )\n",
    "\n",
    "        # Routing decision\n",
    "        chosen = scores.argmax(dim=1)  # [B]\n",
    "\n",
    "        # Costs\n",
    "        costs = 1.0 - tool_preds\n",
    "        chosen_cost = costs[torch.arange(B), chosen]\n",
    "\n",
    "        oracle_cost = costs.masked_fill(tool_mask == 0, 1e9).min(dim=1).values\n",
    "\n",
    "        regret = (chosen_cost - oracle_cost).sum().item()\n",
    "        total_regret += regret\n",
    "        total_samples += B\n",
    "\n",
    "    return total_regret / max(1, total_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7281e639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.30it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] Train Loss: -913.2017 | Val Regret: 0.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.22it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] Train Loss: -879.4879 | Val Regret: 0.2499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.52it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] Train Loss: -838.0365 | Val Regret: 0.2382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:13<00:00,  4.04it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] Train Loss: -980.4909 | Val Regret: 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.24it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] Train Loss: -917.4980 | Val Regret: 0.1943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.57it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05] Train Loss: -853.4120 | Val Regret: 0.2440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.82it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06] Train Loss: -846.7866 | Val Regret: 0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.46it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07] Train Loss: -873.3522 | Val Regret: 0.2120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.43it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08] Train Loss: -865.8953 | Val Regret: 0.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.09it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09] Train Loss: -971.5137 | Val Regret: 0.3454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.16it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: -896.7213 | Val Regret: 0.3619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.26it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: -851.3491 | Val Regret: 0.3524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.41it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: -866.1831 | Val Regret: 0.2070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.37it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: -892.8921 | Val Regret: 0.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.42it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: -921.3738 | Val Regret: 0.2205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.15it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: -932.6384 | Val Regret: 0.1525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.30it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Loss: -934.9215 | Val Regret: 0.1769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.18it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Loss: -923.2560 | Val Regret: 0.2111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:13<00:00,  3.96it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Loss: -979.3263 | Val Regret: 0.1537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.29it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Train Loss: -910.6538 | Val Regret: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.45it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Train Loss: -884.3328 | Val Regret: 0.1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.37it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Train Loss: -891.2880 | Val Regret: 0.2262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.29it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Train Loss: -942.6681 | Val Regret: 0.2312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.29it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Train Loss: -928.8299 | Val Regret: 0.2676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.21it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Train Loss: -938.7985 | Val Regret: 0.2772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.70it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Train Loss: -816.2926 | Val Regret: 0.3574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.45it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Train Loss: -827.5361 | Val Regret: 0.3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.15it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Train Loss: -919.7971 | Val Regret: 0.2692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.44it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Train Loss: -848.9411 | Val Regret: 0.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:12<00:00,  4.25it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Train Loss: -932.5237 | Val Regret: 0.3048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.63it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:04<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Train Loss: -823.0023 | Val Regret: 0.2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.69it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31] Train Loss: -840.5516 | Val Regret: 0.2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 53/53 [00:11<00:00,  4.44it/s]\n",
      "Validation:  18%|█▊        | 3/17 [00:01<00:04,  2.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      4\u001b[39m     train_loss = train_one_epoch(\n\u001b[32m      5\u001b[39m         model, train_loader, ctx_mgr, optimizer, criterion\n\u001b[32m      6\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     val_regret = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     11\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal Regret: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_regret\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/eal2d-t/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, loader, ctx_mgr)\u001b[39m\n\u001b[32m     19\u001b[39m tool_preds = preds_all[:, :, task_idx]\n\u001b[32m     20\u001b[39m tool_mask  = mask_all[:, :, task_idx]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m ctx_img_feat, ctx_gt, ctx_pred = \u001b[43mbuild_context_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m scores = model(\n\u001b[32m     27\u001b[39m     images,\n\u001b[32m     28\u001b[39m     torch.zeros((B, \u001b[32m1\u001b[39m), dtype=torch.long, device=device),\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     tool_mask,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Routing decision\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mbuild_context_tensors\u001b[39m\u001b[34m(ctx_mgr, task_idx, device)\u001b[39m\n\u001b[32m     33\u001b[39m C = ctx_mgr.examples_per_tool\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     ctx = \u001b[43mctx_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;66;03m# No valid context for this tool-task pair\u001b[39;00m\n\u001b[32m     40\u001b[39m         ctx_img_feats.append(\n\u001b[32m     41\u001b[39m             torch.zeros(C, model.img_dim, device=device)\n\u001b[32m     42\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/DySTANce/dev_notebooks/imports.py:307\u001b[39m, in \u001b[36mContextManager.sample_context\u001b[39m\u001b[34m(self, tool_idx, task_idx)\u001b[39m\n\u001b[32m    305\u001b[39m imgs, gt, preds = [], [], []\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    309\u001b[39m     \u001b[38;5;66;03m# Each context triple corresponds to:\u001b[39;00m\n\u001b[32m    310\u001b[39m     \u001b[38;5;66;03m#   image x_b\u001b[39;00m\n\u001b[32m    311\u001b[39m     \u001b[38;5;66;03m#   ground-truth label y_b^t\u001b[39;00m\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m#   tool prediction m_E^t(x_b)\u001b[39;00m\n\u001b[32m    313\u001b[39m     imgs.append(item[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/DySTANce/dev_notebooks/imports.py:153\u001b[39m, in \u001b[36mOpenIRoutedDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    151\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m.records[idx]\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m    155\u001b[39m         img = \u001b[38;5;28mself\u001b[39m.transform(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/eal2d-t/lib/python3.12/site-packages/PIL/Image.py:984\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    982\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/eal2d-t/lib/python3.12/site-packages/PIL/ImageFile.py:280\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         s = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct.error) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    282\u001b[39m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[32m    283\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/eal2d-t/lib/python3.12/site-packages/PIL/JpegImagePlugin.py:414\u001b[39m, in \u001b[36mJpegImageFile.load_read\u001b[39m\u001b[34m(self, read_bytes)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    internal: read more image data\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile.LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_ended\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    417\u001b[39m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[32m    418\u001b[39m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28mself\u001b[39m._ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, ctx_mgr, optimizer, criterion\n",
    "    )\n",
    "\n",
    "    val_regret = evaluate(model, val_loader, ctx_mgr)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch:02d}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Regret: {val_regret:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eal2d-t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
