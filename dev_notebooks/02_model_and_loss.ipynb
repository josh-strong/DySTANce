{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7f41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35d3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the textual instruction q.\n",
    "    For now: simple embedding + mean pooling.\n",
    "    In large-scale settings this would be a frozen LLM / CLIP text encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: [B, T] integer tokens\n",
    "\n",
    "        Returns:\n",
    "            Tensor[B, embed_dim]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(token_ids)          # [B, T, D]\n",
    "        return emb.mean(dim=1)                   # mean pool over tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6781559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANPToolEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Attentive Neural Process encoder for task-conditional tool descriptors.\n",
    "\n",
    "    Builds a representation z_E^t(p) from a small context set D_E^t.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dim: int,\n",
    "        hidden_dim: int,\n",
    "        n_heads: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encodes individual context elements:\n",
    "        # [phi_x(x_b) || y_b^t || m_E^t(x_b)] -> hidden\n",
    "        self.context_proj = nn.Sequential(\n",
    "            nn.Linear(img_dim + 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Self-attention over context elements\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm_ctx = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Cross-attention projections\n",
    "        self.W_Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_K = nn.Linear(img_dim, hidden_dim)\n",
    "        self.W_V = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "        self.norm_out = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_embed: torch.Tensor,\n",
    "        ctx_img_feat: torch.Tensor,\n",
    "        ctx_gt: torch.Tensor,\n",
    "        ctx_pred: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_embed : [B, H]              = u(p)\n",
    "            ctx_img_feat: [M, C, Dx]          = phi_x(x_b)\n",
    "            ctx_gt      : [M, C]              = y_b^t\n",
    "            ctx_pred    : [M, C]              = m_E^t(x_b)\n",
    "\n",
    "        Returns:\n",
    "            z_E : [B, M, H]  task-conditional tool descriptors\n",
    "        \"\"\"\n",
    "        M, C, Dx = ctx_img_feat.shape\n",
    "        B = query_embed.shape[0]\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) Encode context elements\n",
    "        # ------------------------------------------------------------------\n",
    "        ctx_input = torch.cat(\n",
    "            [\n",
    "                ctx_img_feat,\n",
    "                ctx_gt.unsqueeze(-1),\n",
    "                ctx_pred.unsqueeze(-1),\n",
    "            ],\n",
    "            dim=-1\n",
    "        )  # [M, C, Dx+2]\n",
    "\n",
    "        ctx_emb = self.context_proj(ctx_input)  # [M, C, H]\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Self-attention over context (per tool)\n",
    "        # ------------------------------------------------------------------\n",
    "        ctx_emb_sa, _ = self.self_attn(ctx_emb, ctx_emb, ctx_emb)\n",
    "        ctx_emb = self.norm_ctx(ctx_emb + ctx_emb_sa)  # residual\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) Cross-attention: query u(p) attends to each toolâ€™s context\n",
    "        # ------------------------------------------------------------------\n",
    "        Q = self.W_Q(query_embed)                # [B, H]\n",
    "        Q = Q.unsqueeze(1).expand(-1, M, -1)     # [B, M, H]\n",
    "\n",
    "        K = self.W_K(ctx_img_feat)               # [M, C, H]\n",
    "        V = self.W_V(ctx_emb)                    # [M, C, H]\n",
    "\n",
    "        # Attention scores: [B, M, C]\n",
    "        attn_logits = torch.einsum(\n",
    "            \"bmh,mch->bmc\", Q, K\n",
    "        ) * self.scale\n",
    "\n",
    "        attn = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # Weighted sum of values -> [B, M, H]\n",
    "        z = torch.einsum(\"bmc,mch->bmh\", attn, V)\n",
    "\n",
    "        return self.norm_out(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d50b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DySTANceRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Full DySTANce routing model.\n",
    "\n",
    "    Given a query (image, instruction, task) and a panel of tools,\n",
    "    outputs a scalar routing score for each tool.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tasks: int,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Image encoder phi_x\n",
    "        # ------------------------------------------------------------\n",
    "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.img_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.img_dim = 512\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Instruction encoder phi_q\n",
    "        # ------------------------------------------------------------\n",
    "        self.text_encoder = InstructionEncoder(vocab_size, 64)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Task embedding\n",
    "        # ------------------------------------------------------------\n",
    "        self.task_embed = nn.Embedding(num_tasks, 32)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Prompt fusion u(p)\n",
    "        # ------------------------------------------------------------\n",
    "        self.prompt_fusion = nn.Sequential(\n",
    "            nn.Linear(self.img_dim + 64 + 32, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # ANP module psi_E^t(p)\n",
    "        # ------------------------------------------------------------\n",
    "        self.anp = ANPToolEncoder(\n",
    "            img_dim=self.img_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Router head g_theta\n",
    "        # ------------------------------------------------------------\n",
    "        # Input: [u(p) || z_E || m_E^t(x)]\n",
    "        self.router_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + 1, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def extract_img_feat(self, images: torch.Tensor):\n",
    "        return self.img_encoder(images).flatten(1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        text_tokens: torch.Tensor,\n",
    "        task_idx: torch.Tensor,\n",
    "        tool_preds: torch.Tensor,\n",
    "        ctx_img_feat: torch.Tensor,\n",
    "        ctx_gt: torch.Tensor,\n",
    "        ctx_pred: torch.Tensor,\n",
    "        tool_mask: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images     : [B, 3, H, W]\n",
    "            text_tokens: [B, T]\n",
    "            task_idx   : [B]\n",
    "            tool_preds : [B, M]          m_E^t(x)\n",
    "            ctx_img_feat: [M, C, Dx]\n",
    "            ctx_gt     : [M, C]\n",
    "            ctx_pred   : [M, C]\n",
    "            tool_mask  : [B, M]          1 if tool supports task\n",
    "\n",
    "        Returns:\n",
    "            scores : [B, M]\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Build u(p)\n",
    "        # ------------------------------------------------------------\n",
    "        img_feat = self.extract_img_feat(images)\n",
    "        txt_feat = self.text_encoder(text_tokens)\n",
    "        task_feat = self.task_embed(task_idx)\n",
    "\n",
    "        u_p = torch.cat([img_feat, txt_feat, task_feat], dim=-1)\n",
    "        u_p = self.prompt_fusion(u_p)  # [B, H]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Tool descriptors z_E^t(p)\n",
    "        # ------------------------------------------------------------\n",
    "        z_E = self.anp(u_p, ctx_img_feat, ctx_gt, ctx_pred)  # [B, M, H]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Router head\n",
    "        # ------------------------------------------------------------\n",
    "        u_exp = u_p.unsqueeze(1).expand(-1, z_E.size(1), -1)\n",
    "        tool_preds = tool_preds.unsqueeze(-1)\n",
    "\n",
    "        router_in = torch.cat([u_exp, z_E, tool_preds], dim=-1)\n",
    "        scores = self.router_head(router_in).squeeze(-1)  # [B, M]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Hard mask invalid tools\n",
    "        # ------------------------------------------------------------\n",
    "        scores = scores.masked_fill(tool_mask == 0, -1e9)\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d2c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DySTANceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Population comp-sum surrogate loss for DySTANce routing.\n",
    "\n",
    "    This loss:\n",
    "    - supports soft costs in [0,1]\n",
    "    - allows multiple near-optimal tools\n",
    "    - handles variable panel sizes\n",
    "    - is compatible with the theory in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        surrogate_type: str = \"logistic\",\n",
    "        lambda_entropy: float = 0.05,\n",
    "        eps: float = 1e-7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.surrogate_type = surrogate_type\n",
    "        self.lambda_entropy = lambda_entropy\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        router_logits: torch.Tensor,  # [B, M]\n",
    "        tool_costs: torch.Tensor,     # [B, M] in [0,1]\n",
    "        validity_mask: torch.Tensor,  # [B, M] in {0,1}\n",
    "    ):\n",
    "        B, M = router_logits.shape\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 1) Masked softmax over valid tools\n",
    "        # ------------------------------------------------------------\n",
    "        masked_logits = router_logits.masked_fill(validity_mask == 0, -1e9)\n",
    "        pi = F.softmax(masked_logits, dim=1)  # [B, M]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 2) Effective panel size per sample\n",
    "        # ------------------------------------------------------------\n",
    "        m_eff = validity_mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 3) Cost centering (KEY FIX)\n",
    "        # ------------------------------------------------------------\n",
    "        # This allows multiple tools to be \"correct\"\n",
    "        # and stabilizes comp-sum weights for soft costs.\n",
    "        active_costs = tool_costs * validity_mask\n",
    "        min_cost, _ = torch.min(\n",
    "            active_costs + (1 - validity_mask) * 1e9,\n",
    "            dim=1,\n",
    "            keepdim=True,\n",
    "        )\n",
    "        centered_costs = active_costs - min_cost  # >= 0\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 4) Comp-sum weights\n",
    "        # w_j = sum_{k!=j} c_k - m + 2\n",
    "        # ------------------------------------------------------------\n",
    "        sum_costs = centered_costs.sum(dim=1, keepdim=True)\n",
    "        w = (sum_costs - centered_costs) - m_eff + 2.0\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 5) Surrogate Psi(pi)\n",
    "        # ------------------------------------------------------------\n",
    "        if self.surrogate_type == \"logistic\":\n",
    "            psi = -torch.log(pi + self.eps)\n",
    "        elif self.surrogate_type == \"mae\":\n",
    "            psi = 1.0 - pi\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown surrogate: {self.surrogate_type}\")\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 6) Aggregate comp-sum loss\n",
    "        # ------------------------------------------------------------\n",
    "        loss_per_sample = (w * psi * validity_mask).sum(dim=1)\n",
    "        loss_main = loss_per_sample.mean()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 7) Entropy regularization (panel-normalized)\n",
    "        # ------------------------------------------------------------\n",
    "        log_pi = torch.log(pi + self.eps)\n",
    "        entropy = -(pi * log_pi).sum(dim=1)\n",
    "        loss_entropy = -self.lambda_entropy * entropy.mean()\n",
    "\n",
    "        total_loss = loss_main + loss_entropy\n",
    "\n",
    "        return total_loss, {\n",
    "            \"loss_main\": loss_main.item(),\n",
    "            \"loss_entropy\": loss_entropy.item(),\n",
    "            \"avg_panel_size\": m_eff.mean().item(),\n",
    "            \"avg_min_cost\": min_cost.mean().item(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac214ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TESTS\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def masked_softmax(logits, mask):\n",
    "    logits = logits.masked_fill(mask == 0, -1e9)\n",
    "    return F.softmax(logits, dim=-1)\n",
    "\n",
    "def run_loss(\n",
    "    loss_fn,\n",
    "    logits,\n",
    "    costs,\n",
    "    mask,\n",
    "):\n",
    "    logits = logits.clone().requires_grad_(True)\n",
    "    loss, logs = loss_fn(logits, costs, mask)\n",
    "    loss.backward()\n",
    "    return loss.item(), logits.grad.detach(), logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8866e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_single_best():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[0.0, 1.0, 1.0]])  # tool 0 perfect\n",
    "    mask   = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    return logits, costs, mask\n",
    "\n",
    "def scenario_two_good():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[0.0, 0.0, 1.0]])\n",
    "    mask   = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    return logits, costs, mask\n",
    "\n",
    "def scenario_all_bad():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    mask   = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    return logits, costs, mask\n",
    "\n",
    "def scenario_with_mask():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[0.0, 1.0, 1.0]])\n",
    "    mask   = torch.tensor([[1.0, 0.0, 1.0]])\n",
    "    return logits, costs, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1068932",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DySTANceLoss(\n",
    "    surrogate_type=\"logistic\",\n",
    "    lambda_entropy=0.0,  # turn OFF entropy for clarity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd2d74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scenario(name, scenario_fn):\n",
    "    logits, costs, mask = scenario_fn()\n",
    "    loss, grad, logs = run_loss(loss_fn, logits, costs, mask)\n",
    "\n",
    "    pi = masked_softmax(logits, mask)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Costs:\", costs.numpy())\n",
    "    print(\"Probs:\", pi.detach().numpy())\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Gradients:\", grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85c6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single best tool ===\n",
      "Costs: [[0. 1. 1.]]\n",
      "Probs: [[0.33333334 0.33333334 0.33333334]]\n",
      "Loss: 1.0986120700836182\n",
      "Gradients: [[-0.66666645  0.33333325  0.33333325]]\n",
      "\n",
      "=== Two equally good tools ===\n",
      "Costs: [[0. 0. 1.]]\n",
      "Probs: [[0.33333334 0.33333334 0.33333334]]\n",
      "Loss: -1.0986120700836182\n",
      "Gradients: [[-0.33333325 -0.33333325  0.66666645]]\n",
      "\n",
      "=== All tools bad ===\n",
      "Costs: [[1. 1. 1.]]\n",
      "Probs: [[0.33333334 0.33333334 0.33333334]]\n",
      "Loss: -3.2958362102508545\n",
      "Gradients: [[0. 0. 0.]]\n",
      "\n",
      "=== Masked tool ===\n",
      "Costs: [[0. 1. 1.]]\n",
      "Probs: [[0.5 0.  0.5]]\n",
      "Loss: 0.6931469440460205\n",
      "Gradients: [[-0.49999988  0.          0.49999988]]\n"
     ]
    }
   ],
   "source": [
    "test_scenario(\"Single best tool\", scenario_single_best)\n",
    "test_scenario(\"Two equally good tools\", scenario_two_good)\n",
    "test_scenario(\"All tools bad\", scenario_all_bad)\n",
    "test_scenario(\"Masked tool\", scenario_with_mask)\n",
    "\n",
    "## pass, pass, pass, pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a36ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eal2d-t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
