{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7f41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35d3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't really needed at this point\n",
    "# for our project, we’re assuming that the user inputs an image and a question and that \n",
    "# this is 100% always correctly routed to the right pool of tools (like an image and they say “segment” so it routes to the segmentation pool of tools)\n",
    "\n",
    "class InstructionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the textual instruction q.\n",
    "    For now: simple embedding + mean pooling.\n",
    "    In large-scale settings this would be a frozen LLM / CLIP text encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: [B, T] integer tokens\n",
    "\n",
    "        Returns:\n",
    "            Tensor[B, embed_dim]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(token_ids)          # [B, T, D]\n",
    "        return emb.mean(dim=1)                   # mean pool over tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6781559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANPToolEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Attentive Neural Process encoder for task-conditional tool descriptors.\n",
    "\n",
    "    Builds a representation z_E^t(p) from a small context set D_E^t.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dim: int,\n",
    "        hidden_dim: int,\n",
    "        n_heads: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encodes individual context elements:\n",
    "        # [phi_x(x_b) || y_b^t || m_E^t(x_b)] -> hidden\n",
    "        self.context_proj = nn.Sequential(\n",
    "            nn.Linear(img_dim + 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Self-attention over context elements\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm_ctx = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Cross-attention projections\n",
    "        self.W_Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_K = nn.Linear(img_dim, hidden_dim)\n",
    "        self.W_V = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "        self.norm_out = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_embed: torch.Tensor,\n",
    "        ctx_img_feat: torch.Tensor,\n",
    "        ctx_gt: torch.Tensor,\n",
    "        ctx_pred: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_embed : [B, H]              = u(p)\n",
    "            ctx_img_feat: [M, C, Dx]          = phi_x(x_b)\n",
    "            ctx_gt      : [M, C]              = y_b^t\n",
    "            ctx_pred    : [M, C]              = m_E^t(x_b)\n",
    "\n",
    "        Returns:\n",
    "            z_E : [B, M, H]  task-conditional tool descriptors\n",
    "        \"\"\"\n",
    "        M, C, Dx = ctx_img_feat.shape\n",
    "        B = query_embed.shape[0]\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) Encode context elements\n",
    "        # ------------------------------------------------------------------\n",
    "        ctx_input = torch.cat(\n",
    "            [\n",
    "                ctx_img_feat,\n",
    "                ctx_gt.unsqueeze(-1),\n",
    "                ctx_pred.unsqueeze(-1),\n",
    "            ],\n",
    "            dim=-1\n",
    "        )  # [M, C, Dx+2]\n",
    "\n",
    "        ctx_emb = self.context_proj(ctx_input)  # [M, C, H]\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Self-attention over context (per tool)\n",
    "        # ------------------------------------------------------------------\n",
    "        ctx_emb_sa, _ = self.self_attn(ctx_emb, ctx_emb, ctx_emb)\n",
    "        ctx_emb = self.norm_ctx(ctx_emb + ctx_emb_sa)  # residual\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) Cross-attention: query u(p) attends to each tool’s context\n",
    "        # ------------------------------------------------------------------\n",
    "        Q = self.W_Q(query_embed)                # [B, H]\n",
    "        Q = Q.unsqueeze(1).expand(-1, M, -1)     # [B, M, H]\n",
    "\n",
    "        K = self.W_K(ctx_img_feat)               # [M, C, H]\n",
    "        V = self.W_V(ctx_emb)                    # [M, C, H]\n",
    "\n",
    "        # Attention scores: [B, M, C]\n",
    "        attn_logits = torch.einsum(\n",
    "            \"bmh,mch->bmc\", Q, K\n",
    "        ) * self.scale\n",
    "\n",
    "        attn = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # Weighted sum of values -> [B, M, H]\n",
    "        z = torch.einsum(\"bmc,mch->bmh\", attn, V)\n",
    "\n",
    "        return self.norm_out(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d50b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DySTANceRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Full DySTANce routing model.\n",
    "\n",
    "    Given a query (image, instruction, task(?)) and a panel of tools,\n",
    "    outputs a scalar routing score for each tool.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tasks: int,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Image encoder phi_x\n",
    "        # ------------------------------------------------------------\n",
    "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.img_encoder = nn.Sequential(*list(resnet.children())[:-1]) # everything except the last layer (head)\n",
    "        self.img_dim = 512\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Instruction encoder phi_q\n",
    "        # ------------------------------------------------------------\n",
    "        ### CHECK THIS! I don't think it's needed ATM\n",
    "        self.text_encoder = InstructionEncoder(vocab_size, 64)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Task embedding\n",
    "        # ------------------------------------------------------------\n",
    "        self.task_embed = nn.Embedding(num_tasks, 32)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Prompt fusion u(p)\n",
    "        # ------------------------------------------------------------\n",
    "        self.prompt_fusion = nn.Sequential(\n",
    "            nn.Linear(self.img_dim + 64 + 32, hidden_dim), # need to fix hard coded hidden dims here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # ANP module psi_E^t(p)\n",
    "        # info: anp is attentive neural process\n",
    "        # ------------------------------------------------------------\n",
    "        self.anp = ANPToolEncoder(\n",
    "            img_dim=self.img_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Router head g_theta\n",
    "        # ------------------------------------------------------------\n",
    "        # Input: [u(p) || z_E || m_E^t(x)]\n",
    "        self.router_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + 1, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def extract_img_feat(self, images: torch.Tensor):\n",
    "        return self.img_encoder(images).flatten(1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        text_tokens: torch.Tensor,\n",
    "        task_idx: torch.Tensor,\n",
    "        tool_preds: torch.Tensor,\n",
    "        ctx_img_feat: torch.Tensor,\n",
    "        ctx_gt: torch.Tensor,\n",
    "        ctx_pred: torch.Tensor,\n",
    "        tool_mask: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images     : [B, 3, H, W]\n",
    "            text_tokens: [B, T]\n",
    "            task_idx   : [B] (pathology)\n",
    "            tool_preds : [B, M]          m_E^t(x)\n",
    "            ctx_img_feat: [M, C, Dx]\n",
    "            ctx_gt     : [M, C]\n",
    "            ctx_pred   : [M, C]\n",
    "            tool_mask  : [B, M]          1 if tool supports task\n",
    "\n",
    "        Returns:\n",
    "            scores : [B, M]\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Build u(p) i.e. the prompt fusion - embedded information of what is being asked (image, question, pathology(?))\n",
    "        # ------------------------------------------------------------\n",
    "        img_feat = self.extract_img_feat(images)\n",
    "        txt_feat = self.text_encoder(text_tokens)\n",
    "        task_feat = self.task_embed(task_idx)   # the best tool for the task (pathology/label) depends on the label\n",
    "\n",
    "        # concat the image features, text features, and task features\n",
    "        # prompt fusion is information on what is being asked of the system \n",
    "        u_p = torch.cat([img_feat, txt_feat, task_feat], dim=-1)\n",
    "        u_p = self.prompt_fusion(u_p)  # [B, H]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Tool descriptors z_E^t(p)\n",
    "        # ------------------------------------------------------------\n",
    "\n",
    "        # attentive neural process with:\n",
    "        # query: u(p) (prompt fusion above)\n",
    "        # context: ctx_img_feat, ctx_gt, ctx_pred\n",
    "        z_E = self.anp(u_p, ctx_img_feat, ctx_gt, ctx_pred)  # [B, M, H]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Router head\n",
    "        # ------------------------------------------------------------\n",
    "        u_exp = u_p.unsqueeze(1).expand(-1, z_E.size(1), -1)\n",
    "        tool_preds = tool_preds.unsqueeze(-1)\n",
    "\n",
    "        router_in = torch.cat([u_exp, z_E, tool_preds], dim=-1)\n",
    "        scores = self.router_head(router_in).squeeze(-1)  # [B, M]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Hard mask invalid tools\n",
    "        # ------------------------------------------------------------\n",
    "        scores = scores.masked_fill(tool_mask == 0, -1e9)\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6f300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3878215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#### INVESTIGATE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Get repository root directory\n",
    "# Try multiple methods to find the repo root\n",
    "cwd = Path.cwd()\n",
    "if (cwd / 'data').exists():\n",
    "    REPO_ROOT = cwd\n",
    "elif (cwd.parent / 'data').exists():\n",
    "    REPO_ROOT = cwd.parent\n",
    "else:\n",
    "    # Fallback: assume we're in dev_notebooks and go up one level\n",
    "    REPO_ROOT = cwd.parent\n",
    "\n",
    "import imports\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Paths\n",
    "# ------------------------------------------------------------\n",
    "DATA_ROOT = REPO_ROOT / \"data\" / \"openi\"\n",
    "LABELS_DIR = DATA_ROOT / \"labels\"\n",
    "IMAGES_DIR = DATA_ROOT / \"image\"\n",
    "PRED_DIR   = DATA_ROOT / \"predictions\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Labels (tasks)\n",
    "# ------------------------------------------------------------\n",
    "label_names = [\n",
    "    \"Atelectasis\", \"Consolidation\", \"Infiltration\", \"Pneumothorax\",\n",
    "    \"Edema\", \"Emphysema\", \"Fibrosis\", \"Effusion\", \"Pneumonia\",\n",
    "    \"Pleural_Thickening\", \"Cardiomegaly\", \"Nodule\", \"Mass\", \"Hernia\",\n",
    "    \"Lung Lesion\", \"Fracture\", \"Lung Opacity\", \"Enlarged Cardiomediastinum\"\n",
    "]\n",
    "num_tasks = len(label_names)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Tool registry\n",
    "# ------------------------------------------------------------\n",
    "registry_all = imports.scan_prediction_files(str(PRED_DIR))\n",
    "\n",
    "# Example split: train on non-resnet tools\n",
    "train_tools = [t for t in registry_all[\"train\"] if \"resnet\" not in t]\n",
    "\n",
    "train_registry = {t: registry_all[\"train\"][t] for t in train_tools}\n",
    "val_registry   = {t: registry_all[\"val\"][t]   for t in train_tools}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Datasets\n",
    "# ------------------------------------------------------------\n",
    "train_dataset_full = imports.OpenIRoutedDataset(\n",
    "    label_csv=str(LABELS_DIR / \"Train.csv\"),\n",
    "    images_dir=str(IMAGES_DIR),\n",
    "    predictions_registry=train_registry,\n",
    "    label_names=label_names,\n",
    "    transform=None,  # assume tensor conversion inside dataset\n",
    ")\n",
    "\n",
    "val_dataset = imports.OpenIRoutedDataset(\n",
    "    label_csv=str(LABELS_DIR / \"Valid.csv\"),\n",
    "    images_dir=str(IMAGES_DIR),\n",
    "    predictions_registry=val_registry,\n",
    "    label_names=label_names,\n",
    "    transform=None,\n",
    ")\n",
    "\n",
    "ctx_mgr = imports.ContextManager(\n",
    "    dataset=train_dataset_full,\n",
    "    context_fraction=0.1,      # 10% context\n",
    "    examples_per_tool=32,      # B_t\n",
    ")\n",
    "\n",
    "train_dataset = ctx_mgr.routing_dataset()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91530f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d660c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_tensors(ctx_mgr, task_idx, device):\n",
    "    \"\"\"\n",
    "    Builds task-conditional context tensors for all tools.\n",
    "\n",
    "    Returns:\n",
    "        ctx_img_feat : [M, C, Dx] on `device`\n",
    "        ctx_gt       : [M, C]     on `device`\n",
    "        ctx_pred     : [M, C]     on `device`\n",
    "    \"\"\"\n",
    "    ctx_img_feats = []\n",
    "    ctx_gts = []\n",
    "    ctx_preds = []\n",
    "\n",
    "    M = ctx_mgr.dataset.M # number of tools\n",
    "    C = ctx_mgr.examples_per_tool # number of examples\n",
    "\n",
    "    for tool_idx in range(M):\n",
    "        ctx = ctx_mgr.sample_context(tool_idx, task_idx)\n",
    "\n",
    "        if ctx is None:\n",
    "            # No valid context for this tool-task pair\n",
    "            ctx_img_feats.append(\n",
    "                torch.zeros(C, model.img_dim, device=device)\n",
    "            )\n",
    "            ctx_gts.append(\n",
    "                torch.zeros(C, device=device)\n",
    "            )\n",
    "            ctx_preds.append(\n",
    "                torch.zeros(C, device=device)\n",
    "            )\n",
    "        else:\n",
    "            imgs, gt, preds = ctx\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            gt = gt.to(device)\n",
    "            preds = preds.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feats = model.extract_img_feat(imgs)  # [C, Dx]\n",
    "\n",
    "            ctx_img_feats.append(feats)\n",
    "            ctx_gts.append(gt)\n",
    "            ctx_preds.append(preds)\n",
    "\n",
    "    return (\n",
    "        torch.stack(ctx_img_feats, dim=0),  # [M, C, Dx]\n",
    "        torch.stack(ctx_gts, dim=0),        # [M, C]\n",
    "        torch.stack(ctx_preds, dim=0),      # [M, C]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310c4f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "task_idx: 9\n",
      "task_ids: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n",
      "ctx_img_feat.size(): torch.Size([14, 32, 512]) (should be [M, C, Dx])\n",
      "ctx_gt.size(): torch.Size([14, 32]) (should be [M, C])\n",
      "ctx_pred.size(): torch.Size([14, 32]) (should be [M, C])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "model = DySTANceRouter(num_tasks, 1000, 256)\n",
    "model.eval()\n",
    "\n",
    "images = batch[\"image\"].to(device)        # [B, 3, H, W]\n",
    "gt_all = batch[\"gt\"].to(device)           # [B, L] (here L is 18)\n",
    "preds_all = batch[\"tool_preds\"].to(device)  # [B, M, L] (here M is 14 -- num tools train_loader.dataset.dataset.tool_names)\n",
    "mask_all = batch[\"tool_mask\"].to(device)    # [B, M, L] (1 when tool supports task -- see 01_dataloading.ipynb)\n",
    "\n",
    "B = images.size(0)\n",
    "print(B)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Sample a task uniformly -- ATM code works by randomly sampling a task for each batch during training\n",
    "#                            -- this can be quite noisy - probably better to do over all tasks at once and average\n",
    "#                            -- i.e., treat it as a binary relevance problem\n",
    "# ------------------------------------------------------------\n",
    "task_idx = random.randint(0, num_tasks - 1)\n",
    "print(f\"task_idx: {task_idx}\")\n",
    "task_ids = torch.full((B,), task_idx, device=device, dtype=torch.long)\n",
    "print(f\"task_ids: {task_ids}\")\n",
    "\n",
    "# Task-conditional slices\n",
    "gt = gt_all[:, task_idx]                 # [B]\n",
    "tool_preds = preds_all[:, :, task_idx]   # [B, M]\n",
    "tool_mask  = mask_all[:, :, task_idx]    # [B, M]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Build context for this task (see code above)\n",
    "#    -- not very happy with this\n",
    "#    -- it encodes the context images using the feature extractor (same one for the query image)\n",
    "#    -- but it doesn't embed the gt or preds using nn.Embedding() as expected, just concats the 1/0 \n",
    "#            with the embedded query image. L2D-Pop does not do it this way....\n",
    "# ------------------------------------------------------------\n",
    "ctx_img_feat, ctx_gt, ctx_pred = build_context_tensors(\n",
    "    ctx_mgr, task_idx, device\n",
    ")\n",
    "print(f\"ctx_img_feat.size(): {ctx_img_feat.size()} (should be [M, C, Dx])\") # [M, C, Dx]\n",
    "print(f\"ctx_gt.size(): {ctx_gt.size()} (should be [M, C])\") # [M, C]\n",
    "print(f\"ctx_pred.size(): {ctx_pred.size()} (should be [M, C])\") # [M, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86b3833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_feat.size(): torch.Size([16, 512]) (should be [B, Dx])\n",
      "txt_feat.size(): torch.Size([16, 64]) (should be [B, Dq])\n",
      "task_feat.size(): torch.Size([16, 32]) (should be [B, Dt])\n",
      "u_p.size(): torch.Size([16, 608]) (should be [B, Dp+Dq+Dt])\n",
      "embedded_u_p.size(): torch.Size([16, 256]) (should be [B, H])\n"
     ]
    }
   ],
   "source": [
    "### FORWARD LOOP IN TRAINING\n",
    "## this bit just encodes the query information (image, question, pathology(?))\n",
    "\n",
    "#### dealing with the query\n",
    "# ------------------------------------------------------------\n",
    "# Build u(p) i.e. the prompt fusion - embedded information of what is being asked (image, question, pathology(????))\n",
    "# ------------------------------------------------------------\n",
    "img_feat = model.extract_img_feat(images)\n",
    "print(f\"img_feat.size(): {img_feat.size()} (should be [B, Dx])\") # [B, Dx]\n",
    "txt_feat = model.text_encoder(torch.zeros((B, 1), dtype=torch.long, device=device))\n",
    "print(f\"txt_feat.size(): {txt_feat.size()} (should be [B, Dq])\") # [B, Dq]\n",
    "task_feat = model.task_embed(task_ids)   # the best tool for the task (pathology/label) depends on the label\n",
    "print(f\"task_feat.size(): {task_feat.size()} (should be [B, Dt])\") # [B, Dt]\n",
    "\n",
    "# concat the image features, text features, and task features\n",
    "# prompt fusion is information on what is being asked of the system \n",
    "u_p = torch.cat([img_feat, txt_feat, task_feat], dim=-1)\n",
    "print(f\"u_p.size(): {u_p.size()} (should be [B, Dp+Dq+Dt])\") # [B, Dp]\n",
    "embedded_u_p = model.prompt_fusion(u_p)  # [B, H]\n",
    "print(f\"embedded_u_p.size(): {embedded_u_p.size()} (should be [B, H])\") # [B, H]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503ee18",
   "metadata": {},
   "source": [
    "![alt text](l2dpop_anp.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91a4099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 14, C: 32, Dx: 512, B: 16\n",
      "ctx_input.size(): torch.Size([14, 32, 514]) (should be [M, C, Dx+2])\n",
      "ctx_emb.size(): torch.Size([14, 32, 256]) (should be [M, C, H])\n",
      "cntx_emb_sa.size(): torch.Size([14, 32, 256]) (should be [M, C, H])\n",
      "ctx_emb.size(): torch.Size([14, 32, 256]) (should be [M, C, H])\n",
      "z_E.size(): torch.Size([16, 14, 256]) (should be [B, M, H])\n"
     ]
    }
   ],
   "source": [
    "### next bit is the ANP module -- model.anp = ANPToolEncoder\n",
    "\n",
    "#    self.anp = ANPToolEncoder(\n",
    "#             img_dim=self.img_dim,\n",
    "#             hidden_dim=hidden_dim,\n",
    "#         )\n",
    "\n",
    "    # def forward(\n",
    "    #     self,\n",
    "    #     query_embed: torch.Tensor,\n",
    "    #     ctx_img_feat: torch.Tensor,\n",
    "    #     ctx_gt: torch.Tensor,\n",
    "    #     ctx_pred: torch.Tensor,\n",
    "    # ):\n",
    "\n",
    "### ....\n",
    "    #  z_E = self.anp(u_p, ctx_img_feat, ctx_gt, ctx_pred)  # [B, M, H]\n",
    "\n",
    "\n",
    "M, C, Dx = ctx_img_feat.shape\n",
    "B = u_p.shape[0]\n",
    "print(f\"M: {M}, C: {C}, Dx: {Dx}, B: {B}\")\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Encode context elements\n",
    "# ------------------------------------------------------------------\n",
    "ctx_input = torch.cat(\n",
    "    [\n",
    "        ctx_img_feat, # [M, C, Dx]\n",
    "        ctx_gt.unsqueeze(-1), # [M, C, 1]\n",
    "        ctx_pred.unsqueeze(-1), # [M, C, 1]\n",
    "    ],\n",
    "    dim=-1\n",
    ")  # [M, C, Dx+2]\n",
    "print(f\"ctx_input.size(): {ctx_input.size()} (should be [M, C, Dx+2])\") # [M, C, Dx+2]\n",
    "\n",
    "# Encodes individual context elements:\n",
    "# [phi_x(x_b) || y_b^t || m_E^t(x_b)] -> hidden\n",
    "ctx_emb = model.anp.context_proj(ctx_input)  # [M, C, H]\n",
    "print(f\"ctx_emb.size(): {ctx_emb.size()} (should be [M, C, H])\") # [M, C, H]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Self-attention over context (per tool)\n",
    "# The point of self-attention here:\n",
    "# Its purpose is to let context points interact with each other before summarization.\n",
    "#  - Which context points are representative of the expert’s skills\n",
    "#  - Which ones are redundant, noisy, or contradictory\n",
    "#  - Whether there are clusters of competence (e.g., “this expert is good on cars but bad on animals”)\n",
    "#   Without self-attention, each context point is encoded independently and then averaged. \n",
    "#   With self-attention, the embedding of a context point can change depending on what other context points exist.\n",
    "\n",
    "# After SA, we apply an nn.LayerNorm() https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
    "# on ctx_emb + ctx_emb_sa (normalized to a stable scale)\n",
    "# removes the dependency on the number of context points\n",
    "# ------------------------------------------------------------------\n",
    "ctx_emb_sa, _ = model.anp.self_attn(ctx_emb, ctx_emb, ctx_emb)\n",
    "ctx_emb = model.anp.norm_ctx(ctx_emb + ctx_emb_sa)  # residual\n",
    "print(f\"cntx_emb_sa.size(): {ctx_emb_sa.size()} (should be [M, C, H])\") # [M, C, H]\n",
    "print(f\"ctx_emb.size(): {ctx_emb.size()} (should be [M, C, H])\") # [M, C, H]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Cross-attention: query u(p) attends to each tool’s context\n",
    "# ------------------------------------------------------------------\n",
    "Q =  model.anp.W_Q(embedded_u_p)                # [B, H]\n",
    "Q = Q.unsqueeze(1).expand(-1, M, -1)     # [B, M, H]\n",
    "\n",
    "K = model.anp.W_K(ctx_img_feat)               # [M, C, H]\n",
    "V = model.anp.W_V(ctx_emb)                    # [M, C, H]\n",
    "\n",
    "# Attention scores: [B, M, C]\n",
    "attn_logits = torch.einsum(\n",
    "    \"bmh,mch->bmc\", Q, K\n",
    ") * model.anp.scale\n",
    "\n",
    "attn = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "# Weighted sum of values -> [B, M, H]\n",
    "z = torch.einsum(\"bmc,mch->bmh\", attn, V)\n",
    "\n",
    "z_E = model.anp.norm_out(z)\n",
    "print(f\"z_E.size(): {z_E.size()} (should be [B, M, H])\") # [B, M, H]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe4510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_exp.size(): torch.Size([16, 14, 256]) (should be [B, M, Dp+Dq+Dt])\n",
      "tool_preds.size(): torch.Size([16, 14, 1]) (should be [B, M, 1])\n",
      "scores.size(): torch.Size([16, 14]) (should be [B, M])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Router head -- can now condition the rejector on this additional information\n",
    "# ------------------------------------------------------------\n",
    "u_exp = embedded_u_p.unsqueeze(1).expand(-1, z_E.size(1), -1)\n",
    "tool_preds = tool_preds.unsqueeze(-1)\n",
    "print(f\"u_exp.size(): {u_exp.size()} (should be [B, M, Dp+Dq+Dt])\") # [B, M, Dp+Dq+Dt]\n",
    "print(f\"tool_preds.size(): {tool_preds.size()} (should be [B, M, 1])\") # [B, M, 1]\n",
    "\n",
    "router_in = torch.cat([u_exp, z_E, tool_preds], dim=-1)\n",
    "scores = model.router_head(router_in).squeeze(-1)  # [B, M]\n",
    "print(f\"scores.size(): {scores.size()} (should be [B, M])\") # [B, M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1d2c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class DySTANceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Population comp-sum surrogate loss for DySTANce routing.\n",
    "\n",
    "    Improvements vs. earlier version:\n",
    "      - explicit renormalization of pi over valid tools\n",
    "      - entropy computed only over valid tools, optionally normalized by log(panel_size)\n",
    "      - robust handling of degenerate panels\n",
    "      - preserves original comp-sum formulation but applies per-sample cost-centering\n",
    "        (helps allow multiple near-optimal tools)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        surrogate_type: str = \"logistic\",   # \"logistic\" or \"mae\"\n",
    "        lambda_entropy: float = 0.05,\n",
    "        entropy_normalize_by_log: bool = True,  # normalize entropy by log(m_eff)\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.surrogate_type = surrogate_type\n",
    "        self.lambda_entropy = lambda_entropy\n",
    "        self.eps = eps\n",
    "        self.entropy_normalize_by_log = entropy_normalize_by_log\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        router_logits: torch.Tensor,  # [B, M]\n",
    "        tool_costs: torch.Tensor,     # [B, M] in [0,1] (lower better)\n",
    "        validity_mask: torch.Tensor,  # [B, M] in {0,1}\n",
    "    ) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            total_loss: scalar tensor\n",
    "            info: dict of python floats for logging\n",
    "        \"\"\"\n",
    "        B, M = router_logits.shape\n",
    "        device = router_logits.device\n",
    "\n",
    "        # ----------------------------\n",
    "        # 1) Masked softmax -> pi over valid tools\n",
    "        # ----------------------------\n",
    "        very_neg = -1e9\n",
    "        masked_logits = router_logits.masked_fill(validity_mask == 0, very_neg)\n",
    "        pi = F.softmax(masked_logits, dim=1)  # [B, M]\n",
    "\n",
    "        # Explicitly zero-out invalid slots and renormalize to avoid numerical leakage\n",
    "        pi = pi * validity_mask\n",
    "        row_sums = pi.sum(dim=1, keepdim=True)\n",
    "        # If a row has sum==0 (no valid tools), keep a uniform small mass on valid_mask (degenerate).\n",
    "        # But usually row_sums>0; add eps to avoid div by zero.\n",
    "        pi = pi / (row_sums + self.eps)\n",
    "\n",
    "        # ----------------------------\n",
    "        # 2) Effective panel size per sample\n",
    "        # ----------------------------\n",
    "        m_eff = validity_mask.sum(dim=1, keepdim=True)  # [B,1]\n",
    "        # Prevent degenerate panels from causing NaNs later; but we also log this condition.\n",
    "        m_eff_clamped = torch.clamp(m_eff, min=1.0)\n",
    "\n",
    "        # ----------------------------\n",
    "        # 3) Cost centering (subtract per-sample min among active tools)\n",
    "        # ----------------------------\n",
    "        # Zero out invalid costs, then set invalid positions to large +ve so min ignores them.\n",
    "        big = 1e9\n",
    "        active_costs = tool_costs * validity_mask  # zeros at invalid positions\n",
    "        # prepare for min: invalid -> +big so min picks among actual actives\n",
    "        costs_for_min = active_costs + (1.0 - validity_mask) * big\n",
    "        min_cost, _ = torch.min(costs_for_min, dim=1, keepdim=True)  # [B,1], min among actives\n",
    "        # In degenerate rows (no actives) min_cost will be big; clamp to zero for safety\n",
    "        min_cost = torch.where(min_cost > big / 2.0, torch.zeros_like(min_cost), min_cost)\n",
    "\n",
    "        centered_costs = active_costs - min_cost  # now >= 0 (for active positions), invalid remain 0\n",
    "\n",
    "        # Optional: you could also scale by (max-min) to keep ranges bounded, but centering suffices.\n",
    "        # ----------------------------\n",
    "        # 4) Comp-sum weights w_j = sum_{k!=j} c_k - m + 2\n",
    "        # ----------------------------\n",
    "        sum_centered = centered_costs.sum(dim=1, keepdim=True)  # [B,1]\n",
    "        w = (sum_centered - centered_costs) - m_eff_clamped + 2.0  # [B,M]\n",
    "        # w for invalid entries will be masked out downstream\n",
    "\n",
    "        # ----------------------------\n",
    "        # 5) Surrogate Psi(pi)\n",
    "        # ----------------------------\n",
    "        if self.surrogate_type == \"logistic\":\n",
    "            psi = -torch.log(pi + self.eps)\n",
    "        elif self.surrogate_type == \"mae\":\n",
    "            psi = 1.0 - pi\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown surrogate_type={self.surrogate_type}\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # 6) Aggregate comp-sum loss (mask invalid tools)\n",
    "        #    L_i = sum_j w_ij * Psi(pi_ij)  over valid j\n",
    "        # ----------------------------\n",
    "        element = w * psi * validity_mask\n",
    "        loss_per_sample = element.sum(dim=1)  # [B]\n",
    "        loss_main = loss_per_sample.mean()\n",
    "\n",
    "        # ----------------------------\n",
    "        # 7) Entropy regularization (computed over valid tools only)\n",
    "        #    You can normalize entropy per-sample by either m_eff or log(m_eff).\n",
    "        # ----------------------------\n",
    "        # Compute entropy only on valid tools\n",
    "        log_pi = torch.log(pi + self.eps)\n",
    "        entropy_per_row = -(pi * log_pi * validity_mask).sum(dim=1, keepdim=True)  # [B,1]\n",
    "\n",
    "        if self.entropy_normalize_by_log:\n",
    "            # Normalize by log(m_eff) to get a value approx in [0,1] (if m_eff>=2)\n",
    "            # When m_eff==1, log(1)=0 -> avoid dividing by 0; we set denom to 1 in that case.\n",
    "            denom = torch.log(m_eff_clamped + 1e-8)\n",
    "            denom = torch.where(denom == 0.0, torch.ones_like(denom), denom)\n",
    "            entropy_norm = entropy_per_row / (denom + self.eps)\n",
    "        else:\n",
    "            # simple divide by panel size\n",
    "            entropy_norm = entropy_per_row / (m_eff_clamped + self.eps)\n",
    "\n",
    "        loss_entropy = -self.lambda_entropy * entropy_norm.mean()\n",
    "\n",
    "        total_loss = loss_main + loss_entropy\n",
    "\n",
    "        info = {\n",
    "            \"loss_main\": float(loss_main.detach().cpu().item()),\n",
    "            \"loss_entropy\": float(loss_entropy.detach().cpu().item()),\n",
    "            \"avg_panel_size\": float(m_eff.mean().detach().cpu().item()),\n",
    "            \"avg_min_cost\": float(min_cost.mean().detach().cpu().item()),\n",
    "        }\n",
    "\n",
    "        return total_loss, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac214ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TESTS\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def masked_softmax(logits, mask):\n",
    "    logits = logits.masked_fill(mask == 0, -1e9)\n",
    "    return F.softmax(logits, dim=-1)\n",
    "\n",
    "def run_loss(\n",
    "    loss_fn,\n",
    "    logits,\n",
    "    costs,\n",
    "    mask,\n",
    "):\n",
    "    logits = logits.clone().requires_grad_(True)\n",
    "    loss, logs = loss_fn(logits, costs, mask)\n",
    "    loss.backward()\n",
    "    return loss.item(), logits.grad.detach(), logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8866e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_single_best():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[0.0, 1.0, 1.0]])  # tool 0 perfect\n",
    "    mask   = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    return logits, costs, mask\n",
    "\n",
    "def scenario_two_good():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[0.0, 0.0, 1.0]])\n",
    "    mask   = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    return logits, costs, mask\n",
    "\n",
    "def scenario_all_bad():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    mask   = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "    return logits, costs, mask\n",
    "\n",
    "def scenario_with_mask():\n",
    "    logits = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "    costs  = torch.tensor([[0.0, 1.0, 1.0]])\n",
    "    mask   = torch.tensor([[1.0, 0.0, 1.0]])\n",
    "    return logits, costs, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1068932",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DySTANceLoss(\n",
    "    surrogate_type=\"logistic\",\n",
    "    lambda_entropy=0.0,  # turn OFF entropy for clarity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd2d74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scenario(name, scenario_fn):\n",
    "    logits, costs, mask = scenario_fn()\n",
    "    loss, grad, logs = run_loss(loss_fn, logits, costs, mask)\n",
    "\n",
    "    pi = masked_softmax(logits, mask)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Costs:\", costs.numpy())\n",
    "    print(\"Probs:\", pi.detach().numpy())\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Gradients:\", grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85c6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single best tool ===\n",
      "Costs: [[0. 1. 1.]]\n",
      "Probs: [[0.33333334 0.33333334 0.33333334]]\n",
      "Loss: 1.0986120700836182\n",
      "Gradients: [[-0.66666645  0.33333325  0.33333325]]\n",
      "\n",
      "=== Two equally good tools ===\n",
      "Costs: [[0. 0. 1.]]\n",
      "Probs: [[0.33333334 0.33333334 0.33333334]]\n",
      "Loss: -1.0986120700836182\n",
      "Gradients: [[-0.33333325 -0.33333325  0.66666645]]\n",
      "\n",
      "=== All tools bad ===\n",
      "Costs: [[1. 1. 1.]]\n",
      "Probs: [[0.33333334 0.33333334 0.33333334]]\n",
      "Loss: -3.2958362102508545\n",
      "Gradients: [[0. 0. 0.]]\n",
      "\n",
      "=== Masked tool ===\n",
      "Costs: [[0. 1. 1.]]\n",
      "Probs: [[0.5 0.  0.5]]\n",
      "Loss: 0.6931469440460205\n",
      "Gradients: [[-0.49999988  0.          0.49999988]]\n"
     ]
    }
   ],
   "source": [
    "test_scenario(\"Single best tool\", scenario_single_best)\n",
    "test_scenario(\"Two equally good tools\", scenario_two_good)\n",
    "test_scenario(\"All tools bad\", scenario_all_bad)\n",
    "test_scenario(\"Masked tool\", scenario_with_mask)\n",
    "\n",
    "## pass, pass, pass, pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a36ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eal2d-t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
